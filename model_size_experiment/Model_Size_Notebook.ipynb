{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9ec7cee",
   "metadata": {},
   "source": [
    "# Model Size Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf53b145",
   "metadata": {},
   "source": [
    "[Link to ReadMe Section](https://git.cs.vt.edu/sdeepti/facial-expression-recognition#experimenting-different-model-sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe254c5",
   "metadata": {},
   "source": [
    "**Motivation**:\n",
    "Investigate shallower model architecture sizes trained from scratch performance compared to ResNet-50 transfer learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a3bbc",
   "metadata": {},
   "source": [
    "## 1. Initial Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2931f4f8",
   "metadata": {},
   "source": [
    "This adds all the imports that are necessary for the code to run smoothly. It involves importing 'torch' which is necessary to work with our model and retrieve our datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fb68e4",
   "metadata": {},
   "source": [
    "The dataset being used for this experiment is the **KDEF Dataset** which can be found by clicking the following link:\n",
    "https://www.kdef.se/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5720184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 convolutional layers for model size experiment??\n",
    "\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import csv\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "\n",
    "data_dir = 'face_images_80_10_10'\n",
    "print(f'using {data_dir} as data folder')\n",
    "\n",
    "model_save_path = 'FEC_simpleCNN_1_layer' + data_dir + '.pt'\n",
    "\n",
    "num_classes = 7\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46c90ac",
   "metadata": {},
   "source": [
    "## 2. Perform Image Preprocessing and Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178c9f9b",
   "metadata": {},
   "source": [
    "Here we apply the transformations neccessary for the convolutional model to take place. We apply standard preproccessing techniques such as augmenting the resize, rotation, illumination to the images. We also normalize the images to the standard mean and std deviation for transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1a6999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected input size for resnet\n",
    "input_size = 224\n",
    "\n",
    "# transformations to apply to images\n",
    "# data augmentation and normalization for training\n",
    "# just normalization for validation and testing\n",
    "# https://pytorch.org/vision/stable/transforms.html\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(size=(input_size, input_size)),\n",
    "        # transforms.Grayscale(), (cannot use greyscale with resnet)\n",
    "        # rotation augmentation\n",
    "        transforms.RandomRotation(10),\n",
    "        # random flip augmentaion\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        # jitter brightness, contrast, saturation augmentaion\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.1, saturation=0.1, hue=0),\n",
    "        # convert to tensor and normalize\n",
    "        transforms.ToTensor(),\n",
    "        # use ImageNet standard mean and std dev for transfer learning\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(size=(input_size, input_size)),\n",
    "        transforms.ToTensor(),\n",
    "        # use ImageNet standard mean and std dev for transfer learning\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(size=(input_size, input_size)),\n",
    "        transforms.ToTensor(),\n",
    "        # use ImageNet standard mean and std dev for transfer learning\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f74341",
   "metadata": {},
   "source": [
    "Now we initialize the hyperparameter for our models. The hyperparameters we are focusing on are batch size, learning rate, and number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc981ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16\n",
    "#learning_rate = 0.0005\n",
    "learning_rate = 0.005\n",
    "num_epochs = 100 # todo early stopping maybe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4403763a",
   "metadata": {},
   "source": [
    "## 3. Create Training/Validation Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3729de45",
   "metadata": {},
   "source": [
    "We create train and validation datasets which will be used by the CNN model and apply transforms to those datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807f2f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and validation datasets and apply transforms\n",
    "datasets_dict = {dataset_type: ImageFolder(os.path.join(data_dir, dataset_type),\n",
    "    transform=data_transforms[dataset_type]) for dataset_type in ['train', 'val', 'test']}\n",
    "\n",
    "# create train and validation dataloaders\n",
    "dataloaders_dict = {dataset_type: DataLoader(datasets_dict[dataset_type], batch_size=batch_size, shuffle=True, num_workers=4) for dataset_type in ['train', 'val', 'test']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb54903",
   "metadata": {},
   "source": [
    "## 4. Simple Convolutional Neural Network Class\n",
    "\n",
    "In this segment we have our CNN architecture, which has **one** convolutional layer that also goes through the pooling layer.\n",
    "\n",
    "This specific experiment evaluates the performance of 1 CNN Layer, However, various model sizes can be experimented by adding additional Convolution layers in the SimpleCNN class. For example, adding the code below:\n",
    "\n",
    "```python\n",
    "self.conv2 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=8, out_channels=16, kernel_size=5, stride=1, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917a27e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple cnn network architecture\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # dimensions of flat layer using formula from\n",
    "            # https://stackoverflow.com/a/67790132\n",
    "            nn.Linear(100352, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(256, num_classes),\n",
    "            # cross entopy already applies softmax\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1ce0e7",
   "metadata": {},
   "source": [
    "## 5. Function to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496c4858",
   "metadata": {},
   "source": [
    "In this method, we are training and validating our model. We pass in a PyTorch model, dataloaders, loss function, opimizer, and a number of epochs to train and validate for. After iterating through our data, and training and validating through each epoch, we save the model weights and save the best accuracy value we achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training and validation\n",
    "# input: a PyTorch model, a dictionary of dataloaders, a loss function (criterion), \n",
    "#an optimizer, a specified number of epochs to train and validate for\n",
    "def train(model, dataloaders, criterion, optimizer,\n",
    "\t\t\tnum_epochs=25, save_path=None, save_history_to_csv=True):\n",
    "\n",
    "\tsince = time.time()\n",
    "\n",
    "\t# save train and val loss/accuracy history for each epoch\n",
    "\ttrain_loss_history = []\n",
    "\tval_loss_history = []\n",
    "\ttrain_acc_history = []\n",
    "\tval_acc_history = []\n",
    "\n",
    "\n",
    "\tbest_model_wts = copy.deepcopy(model.state_dict())\n",
    "\tbest_acc = 0.0\n",
    "\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tprint(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\t\tprint('-' * 10)\n",
    "\n",
    "\t\t# Each epoch has a training and validation phase\n",
    "\t\tfor phase in ['train', 'val']:\n",
    "\t\t\tif phase == 'train':\n",
    "\t\t\t\tmodel.train()  # Set model to training mode\n",
    "\t\t\telse:\n",
    "\t\t\t\tmodel.eval()   # Set model to evaluate mode\n",
    "\n",
    "\t\t\trunning_loss = 0.0\n",
    "\t\t\trunning_corrects = 0\n",
    "\n",
    "\t\t\t# Iterate over data.\n",
    "\t\t\tfor inputs, labels in dataloaders[phase]:\n",
    "\t\t\t\tinputs = inputs.to(device)\n",
    "\t\t\t\tlabels = labels.to(device)\n",
    "\n",
    "\t\t\t\t# zero the parameter gradients\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\t\t# forward\n",
    "\t\t\t\t# track history if only in train\n",
    "\t\t\t\twith torch.set_grad_enabled(phase == 'train'):\n",
    "\t\t\t\t\t# Get model outputs and calculate loss\n",
    "\t\t\t\t\toutputs = model(inputs)\n",
    "\t\t\t\t\tloss = criterion(outputs, labels)\n",
    "\n",
    "\t\t\t\t\t_, preds = torch.max(outputs, 1)\n",
    "\n",
    "\t\t\t\t\t# backward + optimize only if in training phase\n",
    "\t\t\t\t\tif phase == 'train':\n",
    "\t\t\t\t\t\tloss.backward()\n",
    "\t\t\t\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t\t# statistics\n",
    "\t\t\t\trunning_loss += loss.item() * inputs.size(0)\n",
    "\t\t\t\trunning_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "\t\t\tepoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "\t\t\tepoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "\t\t\tprint(f'{phase} loss: {epoch_loss:.4f} acc: {epoch_acc:.4f}')\n",
    "\n",
    "\t\t\t# deep copy the model if best accuracy\n",
    "\t\t\tif phase == 'val' and epoch_acc > best_acc:\n",
    "\t\t\t\tbest_acc = epoch_acc\n",
    "\t\t\t\tbest_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\t\t\t# save loss and accuracy to history\n",
    "\t\t\tif phase == 'train':\n",
    "\t\t\t\ttrain_loss_history.append(epoch_loss)\n",
    "\t\t\t\ttrain_acc_history.append(epoch_acc.item())\n",
    "\t\t\telif phase == 'val':\n",
    "\t\t\t\tval_loss_history.append(epoch_loss)\n",
    "\t\t\t\tval_acc_history.append(epoch_acc.item())\n",
    "\n",
    "\t\tprint()\n",
    "\n",
    "\ttime_elapsed = time.time() - since\n",
    "\tprint(f'Training complete in {(time_elapsed // 60):.0f}m {(time_elapsed % 60):.0f}s')\n",
    "\tprint(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "\t# load best model weights\n",
    "\tmodel.load_state_dict(best_model_wts)\n",
    "\n",
    "\t# write history csv file\n",
    "\tif save_history_to_csv:\n",
    "\t\thistory_header = ['train_loss', 'val_loss', 'train_acc', 'val_acc']\n",
    "\t\thistory_filename = model_save_path.split('.')[0] + '_history.csv'\n",
    "\t\thistory = zip(train_loss_history, val_loss_history, train_acc_history, val_acc_history)\n",
    "\t\thistory = [list(row) for row in history]\n",
    "\t\twith open(history_filename, 'w') as csv_file:\n",
    "\t\t\twriter = csv.writer(csv_file)\n",
    "\t\t\twriter.writerow(history_header)\n",
    "\t\t\tfor row in history:\n",
    "\t\t\t\twriter.writerow(row)\n",
    "\n",
    "\t\t# save trained model to disk\n",
    "\t\tif save_path:\n",
    "\t\t\ttorch.save(model.state_dict(), save_path)\n",
    "\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35470477",
   "metadata": {},
   "source": [
    "## 6. Function to test our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675dc1b9",
   "metadata": {},
   "source": [
    "Previously we trained our CNN model with our data, now we test our model on our test dataset. We pass in the pytorch model and the test dataset. We create predicted and actual labels and use PyTorch's library to perform the testing phase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588df5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests performance on test set and computes metrics\n",
    "def test(model, test_loader):\n",
    "\t# list of predicted labels of all batches\n",
    "\tpredicted_labels = torch.zeros(0, dtype=torch.long, device='cpu')\n",
    "\t# list of actual labels of all batches\n",
    "\tactual_labels = torch.zeros(0, dtype=torch.long, device='cpu')\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tmodel.eval()\n",
    "\t\t# get batch of inputs (image) and outputs (expression label) from test_loader\n",
    "\t\tfor inputs, labels in test_loader:\n",
    "\t\t\tinputs = inputs.to(device)\n",
    "\t\t\tlabels = labels.to(device)\n",
    "\n",
    "\t\t\t# use model to predict label\n",
    "\t\t\toutputs = model(inputs)\n",
    "\t\t\t_, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "\t\t\t# append batch prediction labels and actual labels\n",
    "\t\t\tpredicted_labels = torch.cat([predicted_labels, preds.view(-1).cpu()])\n",
    "\t\t\tactual_labels = torch.cat([actual_labels, labels.view(-1).cpu()])\n",
    "\n",
    "\tprint('\\nTest Metrics:')\n",
    "\t# print confusion matrix\n",
    "\tprint('Confusion Matrix:')\n",
    "\tprint(confusion_matrix(actual_labels.numpy(), predicted_labels.numpy()))\n",
    "\n",
    "\tprint('Test Accuracy:', accuracy_score(actual_labels.numpy(), predicted_labels.numpy()))\n",
    "\tprint('F1 score:', f1_score(actual_labels.numpy(), predicted_labels.numpy(), average='weighted'))\n",
    "\t# print classification report\n",
    "\tprint('Classification Report:')\n",
    "\tprint(classification_report(actual_labels.numpy(), predicted_labels.numpy()))\n",
    "\n",
    "\treturn predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfbfdeb",
   "metadata": {},
   "source": [
    "## 7. Print model summary and model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceb5dd2",
   "metadata": {},
   "source": [
    "Here we print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2aebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN()\n",
    "# transfer model to gpu if available\n",
    "model = model.to(device)\n",
    "\n",
    "print('Model summary:')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255c6c6b",
   "metadata": {},
   "source": [
    "Here we print the model parameter as well as the trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce0360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print model parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_names = [name for name, p in model.named_parameters() if p.requires_grad]\n",
    "total_trainable_params = len(trainable_names)\n",
    "print()\n",
    "print(f'Model parameters ({total_params} total, {total_trainable_params} trainable)')\n",
    "print('List of trainable parameters:')\n",
    "pprint(trainable_names, width=80, compact=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa1eef8",
   "metadata": {},
   "source": [
    "## 8. Call to test and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019d8159",
   "metadata": {},
   "source": [
    "Lastly, we call the functions we created above. We set the optimizer and loss function.\n",
    "Then call the train and test functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15673660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set optimizer\n",
    "#optimizer = torch.optim.Adam(params_to_update, lr=learning_rate)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# set loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# train model\n",
    "trained_model = train(model, dataloaders_dict, criterion, optimizer,\n",
    "\t\tnum_epochs, model_save_path)\n",
    "\n",
    "# test model\n",
    "test(trained_model, dataloaders_dict['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecf5a1e",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270f72bc",
   "metadata": {},
   "source": [
    "The result obtained from model size experiment is as plotted below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327dd354",
   "metadata": {},
   "source": [
    "<img src=\"https://git.cs.vt.edu/sdeepti/facial-expression-recognition/-/raw/main/Images/1-layer-Res.png">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b9b925",
   "metadata": {},
   "source": [
    "According to our findings, the accuracy of the shallower model architecture that was trained from scratch performed significantly lower than our main ResNet50 classifier. Which is expected because CNN models take longer to converge because they were trained from scratch.\n",
    "\n",
    "If we explore with higher number of convolution layers such as 2, 6, and 12 CNN layers, we find that increase in CNN layers increases the amount of trainable parameters, leading to the accuracy rate to improve. This is also expected because CNN models took longer to converge because they were trained from scratch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
