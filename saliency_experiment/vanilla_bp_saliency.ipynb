{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Backpropagation Saliency Map "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link to Readme section:    \n",
    "\n",
    "https://git.cs.vt.edu/sdeepti/facial-expression-recognition/-/blob/main/README.md#saliency-maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Citations:\n",
    "\n",
    "- https://towardsdatascience.com/saliency-map-using-pytorch-68270fe45e80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**: A deep Learning model is a black-box model. It means that we cannot analyze how the model can predict the result based on the data. As the model gets more complex, the interpretability of the model will reduce. However, we still can infer the deep learning model through a visualization known as a saliency map. Saliency maps are a way to measure the spatial support of a particular class in each image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Initial Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This adds all the imports that are necessary for the code to run smoothly. It involves importing `torch` which is necessary to work with our model and retrieve our datasets. Additionally, `matplotlib.pyplot` is imported in order to visualize the saliency map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import csv\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we set up the model by loading the trained ResNet50 model and getting it set up to be on 'eval' mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/face_images_80_10_10'\n",
    "print(f'using {data_dir} as data folder')\n",
    "\n",
    "num_classes = 7\n",
    "\n",
    "# print if running on gpu or on cpu\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "# load trained model\n",
    "model = models.resnet50(num_classes=num_classes)\n",
    "model.load_state_dict(torch.load('main_resnet50/FEC_resnet50_trained_model.pt', map_location=device))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Set the model on Eval Mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Perform Image Pre-Processing and Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performs the desired pre-processing and data augmentation steps. It splits the necessary transformations based on whether the image is used for training, validation or testing. \n",
    "\n",
    "The training images are resized, having arbitrary rotations added and random horizontal flips. They are also altered by varying their brightness, contrast and saturation values. They are lastly normalizd as per the ImageNet standard.\n",
    "\n",
    "The validation and testing images are only resized and normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations to apply to images\n",
    "# data augmentation and normalization for training\n",
    "# just normalization for validation and testing\n",
    "# https://pytorch.org/vision/stable/transforms.html\n",
    "input_size = 224\n",
    "data_transforms = {\n",
    "\t'train': transforms.Compose([\n",
    "\t\ttransforms.Resize(size=(input_size, input_size)),\n",
    "\t\t# transforms.Grayscale(), (cannot use greyscale with resnet)\n",
    "\t\t# rotation augmentation\n",
    "\t\ttransforms.RandomRotation(10),\n",
    "\t\t# random flip augmentaion\n",
    "\t\ttransforms.RandomHorizontalFlip(),\n",
    "\t\t# jitter brightness, contrast, saturation augmentaion\n",
    "\t\ttransforms.ColorJitter(brightness=0.2, contrast=0.1, saturation=0.1, hue=0),\n",
    "\t\t# convert to tensor and normalize\n",
    "\t\ttransforms.ToTensor(),\n",
    "\t\t# use ImageNet standard mean and std dev for transfer learning\n",
    "\t\ttransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\t]),\n",
    "\t'val': transforms.Compose([\n",
    "\t\ttransforms.Resize(size=(input_size, input_size)),\n",
    "\t\ttransforms.ToTensor(),\n",
    "\t\t# use ImageNet standard mean and std dev for transfer learning\n",
    "\t\ttransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\t]),\n",
    "\t'test': transforms.Compose([\n",
    "\t\ttransforms.Resize(size=(input_size, input_size)),\n",
    "\t\ttransforms.ToTensor(),\n",
    "\t\t# use ImageNet standard mean and std dev for transfer learning\n",
    "\t\ttransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\t])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Perform Backpropogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we transform the image, we have to reshape it because our model reads the tensor on 4-dimensional shape (batch size, channel, width, height). Then, we have to set the image to catch gradient when we do backpropagation to it. Once we do this, we can catch the gradient by put the image on the model and do the backpropagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open sample image \n",
    "image = Image.open('data/face_images_unsplit/happy/AF01HAHR.JPG')\n",
    "\n",
    "# apply transforms the image\n",
    "image = data_transforms['test'](image)\n",
    "\n",
    "# Reshape the image (because the model use \n",
    "# 4-dimensional tensor (batch_size, channel, width, height))\n",
    "image = image.reshape(1, 3, input_size, input_size)\n",
    "\n",
    "# Set the device for the image\n",
    "image = image.to(device)\n",
    "\n",
    "# # Set the requires_grad_ to the image for retrieving gradients\n",
    "image.requires_grad_()\n",
    "\n",
    "# Retrieve output from the image\n",
    "output = model(image)\n",
    "\n",
    "# Catch the output\n",
    "output_idx = output.argmax()\n",
    "output_max = output[0, output_idx]\n",
    "\n",
    "# Do backpropagation to get the derivative of the output based on the image\n",
    "output_max.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Visualize Saliency Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can visualize the gradient using matplotlib. Before doing that, since the image has three channels to it, we have to take the maximum value from those channels on each pixel position. Finally, we can visualize the result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retireve the saliency map and also pick the maximum value from channels on each pixel.\n",
    "# In this case, we look at dim=1. Recall the shape (batch_size, channel, width, height)\n",
    "saliency, _ = torch.max(image.grad.data.abs(), dim=1)\n",
    "saliency = saliency.reshape(224, 224)\n",
    "\n",
    "# Reshape the image\n",
    "image = image.reshape(-1, 224, 224)\n",
    "\n",
    "# Visualize the image and the saliency map\n",
    "inverse_normalize = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "    std=[1/0.229, 1/0.224, 1/0.255]\n",
    ")\n",
    "# undo normalization for viewing\n",
    "with torch.no_grad():\n",
    "        image = inverse_normalize(image)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(image.cpu().numpy().transpose(1, 2, 0))\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(saliency.cpu(), cmap='hot')\n",
    "ax[1].axis('off')\n",
    "plt.tight_layout()\n",
    "fig.suptitle('The Image and Its Saliency Map')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://git.cs.vt.edu/sdeepti/facial-expression-recognition/-/raw/main/Images/vanilla_saliency_map.png\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
