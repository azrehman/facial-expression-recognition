{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cb94897",
   "metadata": {},
   "source": [
    "# Our ResNet50 Classifier\n",
    "\n",
    "### Link to ReadMe:\n",
    "\n",
    "https://gitlab.cs.vt.edu/sdeepti/facial-expression-recognition/-/tree/main/#baseline-model-vs-our-classifier\n",
    "\n",
    "### Citations:\n",
    "\n",
    "- https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#create-the-optimizer\n",
    "- https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "- https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "- https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html\n",
    "- https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "- https://discuss.pytorch.org/t/discussion-why-normalise-according-to-imagenet-mean-and-std-dev-for-transfer-learning/115670\n",
    "- https://pytorch.org/vision/stable/transforms.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8612aa71",
   "metadata": {},
   "source": [
    "**Motivation**: Our goal is to try to classify the 7 standard expressions which are happy, sad, angry, afraid, disgust, surprised, neutral. We chose the ResNet architecture as it has one of the highest performances. These emotions serve as the foundation for the study of human emotional responses and have numerous applications such as Education, Medicine, Criminal Justice and Public Safety. Hence, we were motivated to see how well our model can perform. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44430c17",
   "metadata": {},
   "source": [
    "#### 1. Initial Set-Up\n",
    "\n",
    "This adds all the imports that are necessary for the code to run smoothly. It involves importing `torch` which is necessary to build, train and test model, and retrieve our datasets. Additionally, `sklearn` is used for evaluation metrics to be reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69623158",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# python libraries\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import csv\n",
    "from pprint import pprint\n",
    "\n",
    "# machine learning libraries\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27f2022",
   "metadata": {},
   "source": [
    "The code below sets up some of the constant parameters that will be used throughout.\n",
    "\n",
    "The dataset being used for this experiment is the **KDEF Dataset** which can be found by clicking the following link:\n",
    "https://www.kdef.se/.\n",
    "The data will be split at a 80/10/10 ratio, where 80% will be used for training the model, 10% for validation, and the last 10% for testing the model.\n",
    "\n",
    "The number of classes are also stored as a constant and if a GPU is available, we will perform training using one as it will be more efficient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e297f9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# path to load KDEF data: use 80-10-10 train-val-test split for main classifer\n",
    "data_dir = 'data/face_images_80_10_10'\n",
    "print(f'using {data_dir} as data folder')\n",
    "\n",
    "# where to save trained model to be loaded for later experiments\n",
    "model_save_path = 'FEC_resnet50_trained_' + data_dir + '.pt'\n",
    "\n",
    "# number of classes in dataset\n",
    "# afraid  angry  disgusted  happy  neutral  sad  surprised\n",
    "num_classes = 7\n",
    "\n",
    "# pytorch: set to cuda gpu device if available for faster training\n",
    "# will use cpu if gpu not available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "# expected input size for resnet\n",
    "input_size = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75619ded",
   "metadata": {},
   "source": [
    "The flag below is will be used to tell model to finetune all layers (if true) or just last classification layer (if false).\n",
    "\n",
    "*Note: We intitally set this to false to see accuracy with fintuning just the last classification layer on ResNet but only obtained an accuracy of 75%. Hence, the model was later trained again wih this flag set to True*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de943cfc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# flag for feature extracting, when true, finetune the entire model\n",
    "# when false, only update reshaped layer parameters (last fully connected classification layer)\n",
    "finetune_all_parmas = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd85bbb7",
   "metadata": {},
   "source": [
    "#### 2. Perform Image Pre-Processing and Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116c8f4a",
   "metadata": {},
   "source": [
    "This performs the desired pre-processing and data augmentation steps. It splits the necessary transformations based on whether the image is used for training, validation or testing. \n",
    "\n",
    "The training images are resized, having arbitrary rotations added and random horizontal flips. They are also altered by varying their brightness, contrast and saturation values. They are lastly normalizd as per the ImageNet standard.\n",
    "\n",
    "The validation and testing images are only resized and normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e23d249c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# transformations to apply to images\n",
    "# data augmentation and normalization for training\n",
    "# just normalization for validation and testing\n",
    "# https://pytorch.org/vision/stable/transforms.html\n",
    "data_transforms = {\n",
    "\t'train': transforms.Compose([\n",
    "\t\ttransforms.Resize(size=(input_size, input_size)),\n",
    "\t\t# transforms.Grayscale(), (cannot use greyscale with resnet)\n",
    "\t\t# rotation augmentation\n",
    "\t\ttransforms.RandomRotation(10),\n",
    "\t\t# random flip augmentaion\n",
    "\t\ttransforms.RandomHorizontalFlip(),\n",
    "\t\t# jitter brightness, contrast, saturation augmentaion\n",
    "\t\ttransforms.ColorJitter(brightness=0.2, contrast=0.1, saturation=0.1, hue=0),\n",
    "\t\t# convert to tensor and normalize\n",
    "\t\ttransforms.ToTensor(),\n",
    "\t\t# use ImageNet standard mean and std dev for transfer learning\n",
    "\t\ttransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\t]),\n",
    "\t'val': transforms.Compose([\n",
    "\t\ttransforms.Resize(size=(input_size, input_size)),\n",
    "\t\ttransforms.ToTensor(),\n",
    "\t\t# use ImageNet standard mean and std dev for transfer learning\n",
    "\t\ttransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\t]),\n",
    "\t'test': transforms.Compose([\n",
    "\t\ttransforms.Resize(size=(input_size, input_size)),\n",
    "\t\ttransforms.ToTensor(),\n",
    "\t\t# use ImageNet standard mean and std dev for transfer learning\n",
    "\t\ttransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\t])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0039b48",
   "metadata": {},
   "source": [
    "#### 3. Create Training & Validation & Testing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fdf25d",
   "metadata": {},
   "source": [
    "The following code sets the batch size and creates the training, validation and testing datasets. It then performs the respective transformations on the images of each dataset and creates data loaders for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e1942be",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16\n",
    "\n",
    "# create train and validation datasets and apply transforms\n",
    "datasets_dict = {dataset_type: ImageFolder(os.path.join(data_dir, dataset_type),\n",
    "\ttransform=data_transforms[dataset_type]) for dataset_type in ['train', 'val', 'test']}\n",
    "\n",
    "# create train and validation dataloaders\n",
    "dataloaders_dict = {dataset_type: DataLoader(datasets_dict[dataset_type], \n",
    "    batch_size=batch_size, shuffle=True)for dataset_type in ['train', 'val', 'test']}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c952157",
   "metadata": {},
   "source": [
    "#### 4. Funcion to Initialize Our Pretrained ResNet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55744343",
   "metadata": {},
   "source": [
    "This code loads a pretrained resnet model of a specified size and marks either all layers as trainable or just the last fully connected classification layer based on the finetune_all_params flag. \n",
    "\n",
    "The line: `model.fc = torch.nn.Linear(model.fc.in_features, num_classes)` is what adds the fully connected classification layer with 7 outputs to the end of the resnet convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b22ceb61",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# initialize a pretrained resnet model\n",
    "def init_model(num_classes, resnet_size, finetune_all_parmas,\n",
    "\t\t\t\tprint_model=True, class_to_idx=None):\n",
    "\n",
    "\tmodel = None\n",
    "\n",
    "\tif resnet_size == 18:\n",
    "\t\tmodel = models.resnet18(pretrained=True)\n",
    "\telif resnet_size == 34:\n",
    "\t\tmodel = models.resnet34(pretrained=True)\n",
    "\telif resnet_size == 50:\n",
    "\t\tmodel = models.resnet50(pretrained=True)\n",
    "\telif resnet_size == 101:\n",
    "\t\tmodel = models.resnet101(pretrained=True)\n",
    "\telif resnet_size == 152:\n",
    "\t\tmodel = models.resnet152(pretrained=True)\n",
    "\telse:\n",
    "\t\traise ValueError(f'Invalid size of {resnet_size} given for resnet size.')\n",
    "\n",
    "\n",
    "\t# sets requires_grad attribute of parameters in model to false if not finetuning all parameters\n",
    "\tif not finetune_all_parmas:\n",
    "\t\t# don't relearn weights when transfer learning\n",
    "\t\tfor param in model.parameters():\n",
    "\t\t\tparam.requires_grad = False\n",
    "\n",
    "\t# when transfer learning, set last layer to be fully connected with num_classes number of outputs\n",
    "\tmodel.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "\t# map classes to indexes\n",
    "\tif class_to_idx:\n",
    "\t\tmodel.class_to_idx = class_to_idx\n",
    "\t\tmodel.idx_to_class = {idx: class_ for class_, idx in model.class_to_idx.items()}\n",
    "\n",
    "\n",
    "\t\t# print model information\n",
    "\tif print_model:\n",
    "\t\t# print model summary\n",
    "\t\tprint()\n",
    "\t\tprint(f'Using resnet size: {resnet_size}')\n",
    "\t\tprint('Model summary:')\n",
    "\t\tprint(model)\n",
    "\n",
    "\t\t# print model parameters\n",
    "\t\ttotal_params = sum(p.numel() for p in model.parameters())\n",
    "\t\ttrainable_names = [name for name, p in model.named_parameters() if p.requires_grad]\n",
    "\t\ttotal_trainable_params = len(trainable_names)\n",
    "\t\tprint()\n",
    "\t\tprint(f'Model parameters ({total_params} total, {total_trainable_params} trainable)')\n",
    "\t\tprint('List of trainable parameters:')\n",
    "\t\tpprint(trainable_names, width=80, compact=True)\n",
    "\t\tprint()\n",
    "\n",
    "\t\t# print mapping for class indicies\n",
    "\t\tif class_to_idx:\n",
    "\t\t\tprint('Model index to class mappings:')\n",
    "\t\t\tprint(model.idx_to_class)\n",
    "\t\t\tprint()\n",
    "\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a68b1c",
   "metadata": {},
   "source": [
    "#### 5. Function to Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4022cca6",
   "metadata": {},
   "source": [
    "The function below performs the training for the model by taking in the model, a dictionary of dataloaders, a loss function, an optimizer, and a specified number of epochs to train and validate for. It saves the train and validation loss and accuracy history for each of the epochs and writes it to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cefc51ce",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# model training and validation\n",
    "def train(model, dataloaders, criterion, optimizer,\n",
    "\t\t\tnum_epochs=25, save_path=None, save_history_to_csv=True):\n",
    "\n",
    "\tsince = time.time()\n",
    "\n",
    "\t# save train and val loss/accuracy history for each epoch\n",
    "\ttrain_loss_history = []\n",
    "\tval_loss_history = []\n",
    "\ttrain_acc_history = []\n",
    "\tval_acc_history = []\n",
    "\n",
    "\n",
    "\tbest_model_wts = copy.deepcopy(model.state_dict())\n",
    "\tbest_acc = 0.0\n",
    "\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tprint(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\t\tprint('-' * 10)\n",
    "\n",
    "\t\t# Each epoch has a training and validation phase\n",
    "\t\tfor phase in ['train', 'val']:\n",
    "\t\t\tif phase == 'train':\n",
    "\t\t\t\tmodel.train()  # Set model to training mode\n",
    "\t\t\telse:\n",
    "\t\t\t\tmodel.eval()   # Set model to evaluate mode\n",
    "\n",
    "\t\t\trunning_loss = 0.0\n",
    "\t\t\trunning_corrects = 0\n",
    "\n",
    "\t\t\t# Iterate over data.\n",
    "\t\t\tfor inputs, labels in dataloaders[phase]:\n",
    "\t\t\t\tinputs = inputs.to(device)\n",
    "\t\t\t\tlabels = labels.to(device)\n",
    "\n",
    "\t\t\t\t# zero the parameter gradients\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\t\t# forward\n",
    "\t\t\t\t# track history if only in train\n",
    "\t\t\t\twith torch.set_grad_enabled(phase == 'train'):\n",
    "\t\t\t\t\t# Get model outputs and calculate loss\n",
    "\t\t\t\t\toutputs = model(inputs)\n",
    "\t\t\t\t\tloss = criterion(outputs, labels)\n",
    "\n",
    "\t\t\t\t\t_, preds = torch.max(outputs, 1)\n",
    "\n",
    "\t\t\t\t\t# backward + optimize only if in training phase\n",
    "\t\t\t\t\tif phase == 'train':\n",
    "\t\t\t\t\t\tloss.backward()\n",
    "\t\t\t\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t\t# statistics\n",
    "\t\t\t\trunning_loss += loss.item() * inputs.size(0)\n",
    "\t\t\t\trunning_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "\t\t\tepoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "\t\t\tepoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "\t\t\tprint(f'{phase} loss: {epoch_loss:.4f} acc: {epoch_acc:.4f}')\n",
    "\n",
    "\t\t\t# deep copy the model if best accuracy\n",
    "\t\t\tif phase == 'val' and epoch_acc > best_acc:\n",
    "\t\t\t\tbest_acc = epoch_acc\n",
    "\t\t\t\tbest_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\t\t\t# save loss and accuracy to history\n",
    "\t\t\tif phase == 'train':\n",
    "\t\t\t\ttrain_loss_history.append(epoch_loss)\n",
    "\t\t\t\ttrain_acc_history.append(epoch_acc.item())\n",
    "\t\t\telif phase == 'val':\n",
    "\t\t\t\tval_loss_history.append(epoch_loss)\n",
    "\t\t\t\tval_acc_history.append(epoch_acc.item())\n",
    "\n",
    "\t\tprint()\n",
    "\n",
    "\ttime_elapsed = time.time() - since\n",
    "\tprint(f'Training complete in {(time_elapsed // 60):.0f}m {(time_elapsed % 60):.0f}s')\n",
    "\tprint(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "\t# load best model weights\n",
    "\tmodel.load_state_dict(best_model_wts)\n",
    "\n",
    "\t# write history csv file\n",
    "\tif save_history_to_csv:\n",
    "\t\thistory_header = ['train_loss', 'val_loss', 'train_acc', 'val_acc']\n",
    "\t\thistory_filename = model_save_path.split('.')[0] + '_history.csv'\n",
    "\t\thistory = zip(train_loss_history, val_loss_history, train_acc_history, val_acc_history)\n",
    "\t\thistory = [list(row) for row in history]\n",
    "\t\twith open(history_filename, 'w') as csv_file:\n",
    "\t\t\twriter = csv.writer(csv_file)\n",
    "\t\t\twriter.writerow(history_header)\n",
    "\t\t\tfor row in history:\n",
    "\t\t\t\twriter.writerow(row)\n",
    "\n",
    "\t\t# save trained model to disk\n",
    "\t\tif save_path:\n",
    "\t\t\ttorch.save(model.state_dict(), save_path)\n",
    "\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baefe15",
   "metadata": {},
   "source": [
    "#### 6. Function to Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24dccec",
   "metadata": {},
   "source": [
    "The following code creates the function used to test the model on the testing set. The function takes a trained model, and a test dataloader. The model will predict the label for each batch in the test dataloader and compare to the actual label. Overall, it evaluates the model's performance and computes metrics that are later used for analysis, such as the accuracy, F1 Score and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d56cffe6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# tests performance on test set and computes metrics\n",
    "def test(model, test_loader):\n",
    "\t# list of predicted labels of all batches\n",
    "\tpredicted_labels = torch.zeros(0, dtype=torch.long, device='cpu')\n",
    "\t# list of actual labels of all batches\n",
    "\tactual_labels = torch.zeros(0, dtype=torch.long, device='cpu')\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tmodel.eval()\n",
    "\t\t# get batch of inputs (image) and outputs (expression label) from test_loader\n",
    "\t\tfor inputs, labels in test_loader:\n",
    "\t\t\tinputs = inputs.to(device)\n",
    "\t\t\tlabels = labels.to(device)\n",
    "\n",
    "\t\t\t# use model to predict label\n",
    "\t\t\toutputs = model(inputs)\n",
    "\t\t\t_, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "\t\t\t# append batch prediction labels and actual labels\n",
    "\t\t\tpredicted_labels = torch.cat([predicted_labels, preds.view(-1).cpu()])\n",
    "\t\t\tactual_labels = torch.cat([actual_labels, labels.view(-1).cpu()])\n",
    "\n",
    "\tprint('\\nTest Metrics:')\n",
    "\t# print confusion matrix\n",
    "\tprint('Confusion Matrix:')\n",
    "\tprint(confusion_matrix(actual_labels.numpy(), predicted_labels.numpy()))\n",
    "\n",
    "\tprint('Test Accuracy:', accuracy_score(actual_labels.numpy(), predicted_labels.numpy()))\n",
    "\tprint('F1 score:', f1_score(actual_labels.numpy(), predicted_labels.numpy(), average='weighted'))\n",
    "\t# print classification report\n",
    "\tprint('Classification Report:')\n",
    "\tprint(classification_report(actual_labels.numpy(), predicted_labels.numpy()))\n",
    "\n",
    "\treturn predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc37b486",
   "metadata": {},
   "source": [
    "#### 7. Initialize the Model as ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a6c15f",
   "metadata": {},
   "source": [
    "Initialize the model to the number of classes (7), ResNet model size (number of layers) (50) and dataloader necessary to train the model with the images that are part of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6566723",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# load pretrained ResNet-50 model\n",
    "model = init_model(num_classes, 50, finetune_all_parmas,\n",
    "\t\t\t\t\tclass_to_idx=datasets_dict['train'].class_to_idx)\n",
    "\n",
    "# transfer model to gpu if available\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e74047d",
   "metadata": {},
   "source": [
    "#### 8. Set Parameters that to be Optimized/Updated\n",
    "We are optimizing both the dense classification layers and the ResNet convolutional base. We are using 50 epochs with a learning rate of 0.0005 and Adam as our chosen optimizer as per our baseline. \n",
    "\n",
    "An optimizer is a function or an algorithm that modifies the attributes of the neural network, such as weights and learning rate. The learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0. The learning rate controls how quickly the model is adapted to the problem.\n",
    "\n",
    "Moreover, as per our literature review, Cross Entropy is one of the best loss functions to use when performing categorical classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05241892",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "learning_rate = 0.0005\n",
    "\n",
    "# set parameters needed to be optimized/updated\n",
    "params_to_update = None\n",
    "if not finetune_all_parmas:\n",
    "\tparams_to_update = [param for param in model.parameters() if param.requires_grad]\n",
    "else:\n",
    "\tparams_to_update = model.parameters()\n",
    "\n",
    "# set optimizer\n",
    "optimizer = torch.optim.Adam(params_to_update, lr=learning_rate)\n",
    "# optimizer = torch.optim.SGD(params_to_update, lr=learning_rate)\n",
    "\n",
    "# set loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c5974b",
   "metadata": {},
   "source": [
    "#### 9. Call to Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723361c9",
   "metadata": {},
   "source": [
    "Call the model to be trained with the above set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2a6b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "trained_model = train(model, dataloaders_dict, criterion, optimizer,\n",
    "\t\tnum_epochs, model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08165731",
   "metadata": {},
   "source": [
    "#### 10. Call to Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d0f080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "test(trained_model, dataloaders_dict['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74225ff4",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba5d631",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://git.cs.vt.edu/sdeepti/facial-expression-recognition/-/raw/main/main_resnet50/main_model_results.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8312be",
   "metadata": {},
   "source": [
    "The results above show how well our model performed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb37303",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://git.cs.vt.edu/sdeepti/facial-expression-recognition/-/raw/main/Images/train_val_graph.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea058321",
   "metadata": {},
   "source": [
    "By applying transfer learning on the ResNet-50 model, we achieve 95.7% accuracy on the KDEF dataset. \n",
    "We trained our model for 50 epochs using similar hyperparameter settings as our baseline paper. Our model was able to converge relatively quickly due to transfer learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb8e6e7",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://git.cs.vt.edu/sdeepti/facial-expression-recognition/-/raw/main/main_resnet50/confusion_matrix.png\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
