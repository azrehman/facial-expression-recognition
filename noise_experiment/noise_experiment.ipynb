{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Noise Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link to Readme section: \n",
    "\n",
    "https://git.cs.vt.edu/sdeepti/facial-expression-recognition/-/blob/main/README.md#introducing-noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Citations:\n",
    "\n",
    "- https://discuss.pytorch.org/t/how-to-add-noise-to-mnist-dataset-when-using-pytorch/59745"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**: Noisy images are actually more representative of real world data, which are normally not uniform and often contain many confounding details. Thus, our goal for this experiment was to evaluate our model's performance on test images containing varying levels of noise.\n",
    "\n",
    "This was achieved by applying Gaussian Noise with different levels of variance on our test set. We predict that if our model is robust, then peformance should not decrease, unless a really large amount of noise is applied to our test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Initial Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This adds all the imports that are necessary for the code to run smoothly. It involves importing 'torch' which is necessary to work with our model and retrieve our datasets. Additionally, 'sklearn' is used for evaluation metrics to be reported. Note that we are importing 'skimage.util' to utilize random noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "\n",
    "from skimage.util import random_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset being used is the **KDEF Dataset** which can be found by clicking the following link:\n",
    "https://www.kdef.se/ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment, we will be analyze how our model's performance varies when different levels of noise (different values of variance) are applied to our test set. Thus, the variable **variance** will be modified when needing to change the level of noise applied to the dataset. Initially we set it to 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = 0.05\n",
    "print(f'using variance of {variance}')\n",
    "\n",
    "model_path = '../main_resnet50/FEC_resnet50_trained_face_images_80_10_10.pt'\n",
    "# model_path = '../dataset_size_experiment/dataset_size_70/FEC_resnet50_trained_face_images_70_10_20.pt'\n",
    "data_dir = '../data/face_images_80_10_10'\n",
    "num_classes = 7\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to load the trained model. Transfer the model to a GPU if avaliable, and then set the model to evaluation mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model\n",
    "model = models.resnet50(num_classes=num_classes)\n",
    "# transfer model to gpu if available\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "# set model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Noise Transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our custom noise transformation to add Gaussian Noise to our test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNoise(object):\n",
    "\tdef __init__(self, mean=0., var=1.):\n",
    "\t\tself.var = var\n",
    "\t\tself.mean = mean\n",
    "\n",
    "\tdef __call__(self, tensor):\n",
    "\t\treturn torch.tensor(random_noise(tensor, mode='gaussian', mean=self.mean, var=self.var, clip=True), dtype=torch.float)\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn self.__class__.__name__ + f'(mean={self.mean}, var={self.var})'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the Gaussian Noise along with the other transformations applied to the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test transformations with noise added\n",
    "test_transforms = transforms.Compose([\n",
    "\ttransforms.Resize(size=(224, 224)),\n",
    "\ttransforms.ToTensor(),\n",
    "\t# use ImageNet standard mean and std dev for transfer learning\n",
    "\ttransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    \t# add noise after normalization\n",
    "\tGaussianNoise(mean=0, var=variance)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Load Test Dataset and Create Dataloader\n",
    "\n",
    "Now we load our test dataset to which we applied transformations, as well as our Gaussian Noise. Then we create the dataloader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset and create dataloader\n",
    "batch_size = 16\n",
    "test_set = datasets.ImageFolder(os.path.join(data_dir, 'test'), transform=test_transforms)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Test Model Performance on Test Set and Compute Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below evaluates our model's performance on the test set with Gaussian Noise added to it (i.e. variance) and computes the metrics we decided to use for all experiments, and prints them. The metrics we are using include a Confusion Matrix, F1 Score, and Classification Report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests performance on test set and computes metrics\n",
    "def test(model, test_loader):\n",
    "\t# list of predicted labels of all batches\n",
    "\tpredicted_labels = torch.zeros(0, dtype=torch.long, device='cpu')\n",
    "\t# list of actual labels of all batches\n",
    "\tactual_labels = torch.zeros(0, dtype=torch.long, device='cpu')\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tmodel.eval()\n",
    "\t\t# get batch of inputs (image) and outputs (expression label) from test_loader\n",
    "\t\tfor inputs, labels in test_loader:\n",
    "\t\t\tinputs = inputs.to(device)\n",
    "\t\t\tlabels = labels.to(device)\n",
    "\n",
    "\t\t\t# use model to predict label\n",
    "\t\t\toutputs = model(inputs)\n",
    "\t\t\t_, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "\t\t\t# append batch prediction labels and actual labels\n",
    "\t\t\tpredicted_labels = torch.cat([predicted_labels, preds.view(-1).cpu()])\n",
    "\t\t\tactual_labels = torch.cat([actual_labels, labels.view(-1).cpu()])\n",
    "\n",
    "\tprint('\\nTest Metrics:')\n",
    "\t# print confusion matrix\n",
    "\tprint('Confusion Matrix:')\n",
    "\tprint(confusion_matrix(actual_labels.numpy(), predicted_labels.numpy()))\n",
    "\n",
    "\tprint('Test Accuracy:', accuracy_score(actual_labels.numpy(), predicted_labels.numpy()))\n",
    "\tprint('F1 score:', f1_score(actual_labels.numpy(), predicted_labels.numpy(), average='weighted'))\n",
    "\t# print classification report\n",
    "\tprint('Classification Report:')\n",
    "\tprint(classification_report(actual_labels.numpy(), predicted_labels.numpy()))\n",
    "\n",
    "\treturn predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following results were obtained for a variance value of 0.01. \n",
    "\n",
    "<div>\n",
    "<img src=\"../Images/noise-exp-var-0.01.png\" width=\"550\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the code for this experiment was ran multiple times for varying values of variance to understand how our model performed when different amounts of noise were applied to our test set. The variance values that we used were: 0.01, 0.05, 0.07, 0.1, 0.15, 0.2. Upon running this experiment for differing values of variance, the following results were obtained and plotted:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://git.cs.vt.edu/sdeepti/facial-expression-recognition/-/raw/main/Images/noise-experiment-line-graph.png\"  width=\"450\" height=\"320\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that there was a gradual decrease in test accuracy for increasing values of variance indicating that our model was not very robust to noise, and would not neccesarily perform well with real-world data. \n",
    "\n",
    "There are multiple techniques we could apply to fix this. An obvious option is to retrain our model with a small random amount of noise added to our training images as a data augmentation. By training with noisy images, our model should be more agnostic to confounding details and perform better on real world images. Another option is to limit overfitting in our model using techniques such as dropout, early stopping, and loss regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Noise Experiment - Statistical Significance Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also performed a Statistical Significance Study on the noise experiment for a variance level of 0.1. We repeated the experiment using the same applied noise to our test set, and evaluated our model on the noisy test set 10 times, and plotted our test accuracies using a box plot as seen below:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://git.cs.vt.edu/sdeepti/facial-expression-recognition/-/raw/main/Images/noise-statistical-sig.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize our findings, the median accuracy of our ten runs was 0.67, the minimum accuracy was 0.65, the maximum was 0.680, and that we have no outliers. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb4e4f248904f6ecbffe6d340b9c348da43b8aab1180ec492e6a4786161b0f7"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
