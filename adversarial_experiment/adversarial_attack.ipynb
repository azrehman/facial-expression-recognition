{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Attack Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link to ReadMe Section:\n",
    "https://gitlab.cs.vt.edu/sdeepti/facial-expression-recognition#adversarial-attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Citations:\n",
    "- https://pytorch.org/tutorials/beginner/fgsm_tutorial.html\n",
    "- https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial10/Adversarial_Attacks.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**:\n",
    "Adversarial machine learning, a technique that attempts to fool models with deceptive data, is a growing threat in the AI and machine learning research community. Therefore, to test our model's robustness, we used Fast Gradient Signed Method (FGSM). FGSM is a white-box attack as it leverages an internal component of the architecture which is its gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Initial Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This adds all the imports that are necessary for the code to run smoothly. It involves importing 'torch' which is necessary to work with our model and retrieve our datasets. Additionally, 'sklearn' is used for evaluation metrics to be reported. On top of these basic imports, we import `torch.nn.functional` in order to use non-linear activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# For metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sets up some of the constant parameters necessary throughout the code such as the pre-trained model path, dataset directory path and more. It also creates the array of epsilon values for which this experiment will investigate on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to trained model\n",
    "model_path = '../main_resnet50/FEC_resnet50_trained_face_images_80_10_10.pt'\n",
    "# directory to dataset\n",
    "data_dir = '../face_images_80_10_10'\n",
    "\n",
    "num_classes = 7\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Mean and Std from ImageNet\n",
    "NORM_MEAN = np.array([0.485, 0.456, 0.406])\n",
    "NORM_STD = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "#List of epsilon values to use for the run.\n",
    "#It is important to keep 0 in the list because it represents the model performance on the original test set.\n",
    "#Also, intuitively we would expect the larger the epsilon, the more noticeable the perturbations\n",
    "#but the more effective the attack in terms of degrading model accuracy.\n",
    "\n",
    "epsilons = [0, .05, .1, .15, .2, .25, .3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Perform Pre-Processing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performs the desired pre-processing steps for the images that belong to the testing dataset. They will be resized and normalized as per the ImageNet standards. The necessary data loader will be created for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms for test set\n",
    "test_transforms = transforms.Compose([\n",
    "\ttransforms.Resize(size=(224, 224)),\n",
    "\ttransforms.ToTensor(),\n",
    "\t# use ImageNet standard mean and std dev for transfer learning\n",
    "\ttransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "batch_size = 32\n",
    "# load test dataset and create dataloader\n",
    "test_set = datasets.ImageFolder(os.path.join(data_dir, 'test'), transform=test_transforms)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#map indexes to class names\n",
    "idx_to_class = {v: k for k, v in test_set.class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create Function to Perform FGSM Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an image, we create an adversarial example by the following expression:\n",
    "\n",
    "*pertubed image = image + epsilon * sign(data_grad)*\n",
    "\n",
    "\n",
    "The term *sign(data_grad)* represents the loss of the network for classifying input image  as label ;  is the intensity of the noise, and \n",
    "the final adversarial example. The equation resembles SGD and is actually nothing else than that. We change the input image  in the direction of maximizing the loss . This is exactly the other way round as during training, where we try to minimize the loss. The sign function and  can be seen as gradient clipping and learning rate specifically. We only allow our attack to change each pixel value by . You can also see that the attack can be performed very fast, as it only requires a single forward and backward pass. The implementation is as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(model, imgs, labels, epsilon):\n",
    "\t# Collect the element-wise sign of the data gradient\n",
    "\tinp_imgs = imgs.clone().requires_grad_()\n",
    "\tpreds = model(inp_imgs.to(device))\n",
    "\tpreds = F.log_softmax(preds, dim=-1)\n",
    "    # Calculate loss by NLL\n",
    "\tloss = -torch.gather(preds, 1, labels.to(device).unsqueeze(dim=-1))\n",
    "\tloss.sum().backward()\n",
    "\t# Update image to adversarial example as written above\n",
    "\tnoise_grad = torch.sign(inp_imgs.grad.to(imgs.device))\n",
    "\tfake_imgs = imgs + epsilon * noise_grad\n",
    "\tfake_imgs.detach_()\n",
    "\treturn fake_imgs, noise_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Function to Visualize Adversarial Images & Model Confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below plots an image (including adversarial images) along with a bar diagram of its predictions for the different possible classes and confidence score. It is visualized by showing the true image first and then the image of the added noise and the perturbed image once that noise is added. On the left, it finally shows the bar diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_fgsm_confidence(model, device, image_batch, label_batch, epsilon):\n",
    "    print(f'epsilon is {epsilon}')\n",
    "    adv_images, noise_grad = fgsm_attack(model, image_batch, label_batch, epsilon)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        adv_preds = model(adv_images.to(device))\n",
    "    for i in range(1, 489, 60):\n",
    "        filename = f'adversarial_sample_{i}_{epsilon}.png'\n",
    "        show_prediction(image_batch[i], label_batch[i], adv_preds[i], filename, epsilon, adv_img=adv_images[i], noise=noise_grad[i])\n",
    "\n",
    "\n",
    "def show_prediction(img, label, pred, filename, epsilon, K=5, adv_img=None, noise=None):\n",
    "\n",
    "\tif isinstance(img, torch.Tensor):\n",
    "\t\t# Tensor image to numpy\n",
    "\t\timg = img.cpu().permute(1, 2, 0).numpy()\n",
    "\t\timg = (img * NORM_STD[None,None]) + NORM_MEAN[None,None]\n",
    "\t\timg = np.clip(img, a_min=0.0, a_max=1.0)\n",
    "\t\tlabel = label.item()\n",
    "\n",
    "\t# Plot on the left the image with the true label as title.\n",
    "\t# On the right, have a horizontal bar plot with the top k predictions including probabilities\n",
    "\tif noise is None or adv_img is None:\n",
    "\t\tfig, ax = plt.subplots(1, 2, figsize=(10,2), gridspec_kw={'width_ratios': [1, 1]})\n",
    "\telse:\n",
    "\t\tfig, ax = plt.subplots(1, 5, figsize=(12,2), gridspec_kw={'width_ratios': [1, 1, 1, 1, 2]})\n",
    "\n",
    "\tax[0].imshow(img)\n",
    "\tax[0].set_title(idx_to_class[label])\n",
    "\tax[0].axis('off')\n",
    "\n",
    "\tif adv_img is not None and noise is not None:\n",
    "\t\t# Visualize adversarial images\n",
    "\t\tadv_img = adv_img.cpu().permute(1, 2, 0).numpy()\n",
    "\t\tadv_img = (adv_img * NORM_STD[None,None]) + NORM_MEAN[None,None]\n",
    "\t\tadv_img = np.clip(adv_img, a_min=0.0, a_max=1.0)\n",
    "\t\tax[1].imshow(adv_img)\n",
    "\t\tax[1].set_title(f'Adversarial (epsilon={epsilon})')\n",
    "\t\tax[1].axis('off')\n",
    "\t\t# Visualize noise\n",
    "\t\tnoise = noise.cpu().permute(1, 2, 0).numpy()\n",
    "\t\tnoise = noise * 0.5 + 0.5 # Scale between 0 to 1\n",
    "\t\tax[2].imshow(noise)\n",
    "\t\tax[2].set_title('Noise')\n",
    "\t\tax[2].axis('off')\n",
    "\t\t# buffer\n",
    "\t\tax[3].axis('off')\n",
    "\n",
    "\tif abs(pred.sum().item() - 1.0) > 1e-4:\n",
    "\t\tpred = torch.softmax(pred, dim=-1)\n",
    "\ttopk_vals, topk_idx = pred.topk(K, dim=-1)\n",
    "\ttopk_vals, topk_idx = topk_vals.cpu().numpy(), topk_idx.cpu().numpy()\n",
    "\tax[-1].barh(np.arange(K), topk_vals*100.0, align='center', color=[\"C0\" if topk_idx[i]!=label else \"C2\" for i in range(K)])\n",
    "\tax[-1].set_yticks(np.arange(K))\n",
    "\tax[-1].set_yticklabels([idx_to_class[c].title() for c in topk_idx])\n",
    "\tax[-1].invert_yaxis()\n",
    "\tax[-1].set_xlabel('Confidence')\n",
    "\tax[-1].set_title('Predictions')\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(filename, bbox_inches='tight')\n",
    "\tplt.show()\n",
    "\t#plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img height=150 src=\"https://git.cs.vt.edu/sdeepti/facial-expression-recognition/-/raw/main/Images/epsilon_0.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img height=150 src=\"https://git.cs.vt.edu/sdeepti/facial-expression-recognition/-/raw/main/Images/epsilon_0.01.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Function to Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important that we verify the performance of our model. Sometimes, simply looking at the accuracy of a model is not sufficient to make clear conclusions about the model's performance. A common alternative metric is “Top-5 accuracy”, which tells us how many times the true label has been within the 5 most-likely predictions of the model. As models usually perform quite well on those, we report the error (1 - accuracy) instead of the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, device, test_loader, img_func=None):\n",
    "    tp, tp_5 = 0.0, 0.0\n",
    "    counter = 0.0\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        if img_func is not None:\n",
    "            images = img_func(images, labels)\n",
    "        with torch.no_grad():\n",
    "            preds = model(images)\n",
    "        tp += (preds.argmax(dim=-1) == labels).sum()\n",
    "        tp_5 += (preds.topk(5, dim=-1)[1] == labels[...,None]).any(dim=-1).sum()\n",
    "        counter += preds.shape[0]\n",
    "    acc = tp.float().item()/counter\n",
    "    top5 = tp_5.float().item()/counter\n",
    "    print(f'Top-1 error: {(100.0 * (1 - acc)):4.2f}%')\n",
    "    print(f\"Top-5 error: {(100.0 * (1 - top5)):4.2f}%\")\n",
    "    return acc, top5\n",
    "\n",
    "\n",
    "print('performance with no attack:')\n",
    "eval_model(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Call to Evaluate the Model on FGSM Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in epsilons:\n",
    "    print(f'evaluating epsilon: {eps}')\n",
    "    _ = eval_model(model, device, test_loader, img_func=lambda x, y: fgsm_attack(model, x, y, epsilon=eps)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://git.cs.vt.edu/sdeepti/facial-expression-recognition/-/raw/main/Images/epsilon_graph_small.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that even a small epsilon value can have quite a drastic impact on the performance of the model. An epsilon value of 0 gives us our original accuracy of 96% but an epsilon of 0.1 drops the accuracy significantly to 4.7%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img height=200 src=\"https://git.cs.vt.edu/sdeepti/facial-expression-recognition/-/raw/main/Images/epsilon_0.3.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, when the epsilon value is too high, such as 0.3, it acts similarly to adding noise to the image.  This is because the model still misclassifies the image but it is no longer confident in one label and is more confused which causes it to have some confidence in numerous labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results further prove that our model might not be the most robust against such white box attacks. However, it is important to note that it is particularly hard to make a model defend itself against these types of attacks since the attacker has access to the model's internal architecture and parameters."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
