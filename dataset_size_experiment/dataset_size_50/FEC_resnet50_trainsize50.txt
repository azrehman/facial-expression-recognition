using face_images_50_10_40 as data folder
Using cuda device

Model summary:
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=7, bias=True)
)

Model parameters (23522375 total, 161 trainable)
List of trainable parameters:
['conv1.weight', 'bn1.weight', 'bn1.bias', 'layer1.0.conv1.weight',
 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.conv2.weight',
 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.0.conv3.weight',
 'layer1.0.bn3.weight', 'layer1.0.bn3.bias', 'layer1.0.downsample.0.weight',
 'layer1.0.downsample.1.weight', 'layer1.0.downsample.1.bias',
 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias',
 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias',
 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.1.bn3.bias',
 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.bn1.bias',
 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.bn2.bias',
 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer1.2.bn3.bias',
 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias',
 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias',
 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.bn3.bias',
 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight',
 'layer2.0.downsample.1.bias', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight',
 'layer2.1.bn1.bias', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight',
 'layer2.1.bn2.bias', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight',
 'layer2.1.bn3.bias', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight',
 'layer2.2.bn1.bias', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight',
 'layer2.2.bn2.bias', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight',
 'layer2.2.bn3.bias', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight',
 'layer2.3.bn1.bias', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight',
 'layer2.3.bn2.bias', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight',
 'layer2.3.bn3.bias', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight',
 'layer3.0.bn1.bias', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight',
 'layer3.0.bn2.bias', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight',
 'layer3.0.bn3.bias', 'layer3.0.downsample.0.weight',
 'layer3.0.downsample.1.weight', 'layer3.0.downsample.1.bias',
 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias',
 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias',
 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.1.bn3.bias',
 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn1.bias',
 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.bn2.bias',
 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.2.bn3.bias',
 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.bn1.bias',
 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.bn2.bias',
 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.3.bn3.bias',
 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.bn1.bias',
 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.bn2.bias',
 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.4.bn3.bias',
 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn1.bias',
 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn2.bias',
 'layer3.5.conv3.weight', 'layer3.5.bn3.weight', 'layer3.5.bn3.bias',
 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.bn1.bias',
 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.bn2.bias',
 'layer4.0.conv3.weight', 'layer4.0.bn3.weight', 'layer4.0.bn3.bias',
 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight',
 'layer4.0.downsample.1.bias', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight',
 'layer4.1.bn1.bias', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight',
 'layer4.1.bn2.bias', 'layer4.1.conv3.weight', 'layer4.1.bn3.weight',
 'layer4.1.bn3.bias', 'layer4.2.conv1.weight', 'layer4.2.bn1.weight',
 'layer4.2.bn1.bias', 'layer4.2.conv2.weight', 'layer4.2.bn2.weight',
 'layer4.2.bn2.bias', 'layer4.2.conv3.weight', 'layer4.2.bn3.weight',
 'layer4.2.bn3.bias', 'fc.weight', 'fc.bias']

Model index to class mappings:
{0: 'afraid', 1: 'angry', 2: 'disgusted', 3: 'happy', 4: 'neutral', 5: 'sad', 6: 'surprised'}

Epoch 1/50
----------
train loss: 1.8291 acc: 0.2482
val loss: 1.5702 acc: 0.4204

Epoch 2/50
----------
train loss: 1.1600 acc: 0.5833
val loss: 1.0750 acc: 0.6082

Epoch 3/50
----------
train loss: 0.7434 acc: 0.7400
val loss: 0.8789 acc: 0.6959

Epoch 4/50
----------
train loss: 0.5500 acc: 0.8061
val loss: 1.0396 acc: 0.6204

Epoch 5/50
----------
train loss: 0.4394 acc: 0.8449
val loss: 0.4619 acc: 0.8449

Epoch 6/50
----------
train loss: 0.3788 acc: 0.8616
val loss: 0.5984 acc: 0.8102

Epoch 7/50
----------
train loss: 0.3348 acc: 0.8829
val loss: 0.4058 acc: 0.8816

Epoch 8/50
----------
train loss: 0.2677 acc: 0.9106
val loss: 0.3493 acc: 0.8857

Epoch 9/50
----------
train loss: 0.2374 acc: 0.9212
val loss: 0.3022 acc: 0.8959

Epoch 10/50
----------
train loss: 0.2093 acc: 0.9273
val loss: 0.4612 acc: 0.8510

Epoch 11/50
----------
train loss: 0.1896 acc: 0.9371
val loss: 0.6961 acc: 0.7816

Epoch 12/50
----------
train loss: 0.1918 acc: 0.9343
val loss: 0.3283 acc: 0.8980

Epoch 13/50
----------
train loss: 0.1445 acc: 0.9506
val loss: 0.2920 acc: 0.9000

Epoch 14/50
----------
train loss: 0.1502 acc: 0.9502
val loss: 0.3217 acc: 0.8959

Epoch 15/50
----------
train loss: 0.1307 acc: 0.9547
val loss: 0.2897 acc: 0.9102

Epoch 16/50
----------
train loss: 0.1368 acc: 0.9551
val loss: 0.3039 acc: 0.9020

Epoch 17/50
----------
train loss: 0.1193 acc: 0.9649
val loss: 0.2579 acc: 0.9204

Epoch 18/50
----------
train loss: 0.0985 acc: 0.9669
val loss: 0.4035 acc: 0.8776

Epoch 19/50
----------
train loss: 0.1031 acc: 0.9661
val loss: 0.4909 acc: 0.8531

Epoch 20/50
----------
train loss: 0.0969 acc: 0.9710
val loss: 2.5575 acc: 0.5041

Epoch 21/50
----------
train loss: 0.1022 acc: 0.9686
val loss: 0.2042 acc: 0.9469

Epoch 22/50
----------
train loss: 0.0655 acc: 0.9792
val loss: 0.6549 acc: 0.8429

Epoch 23/50
----------
train loss: 0.0717 acc: 0.9755
val loss: 0.2322 acc: 0.9408

Epoch 24/50
----------
train loss: 0.0684 acc: 0.9776
val loss: 0.2372 acc: 0.9327

Epoch 25/50
----------
train loss: 0.0701 acc: 0.9771
val loss: 0.2417 acc: 0.9245

Epoch 26/50
----------
train loss: 0.0623 acc: 0.9804
val loss: 0.2104 acc: 0.9449

Epoch 27/50
----------
train loss: 0.0584 acc: 0.9837
val loss: 0.2661 acc: 0.9245

Epoch 28/50
----------
train loss: 0.0434 acc: 0.9861
val loss: 0.1823 acc: 0.9510

Epoch 29/50
----------
train loss: 0.0500 acc: 0.9845
val loss: 0.3381 acc: 0.8980

Epoch 30/50
----------
train loss: 0.0454 acc: 0.9865
val loss: 0.2401 acc: 0.9245

Epoch 31/50
----------
train loss: 0.0354 acc: 0.9886
val loss: 2.2365 acc: 0.6776

Epoch 32/50
----------
train loss: 0.0649 acc: 0.9767
val loss: 0.1828 acc: 0.9571

Epoch 33/50
----------
train loss: 0.0367 acc: 0.9878
val loss: 0.2262 acc: 0.9306

Epoch 34/50
----------
train loss: 0.0353 acc: 0.9898
val loss: 0.1859 acc: 0.9510

Epoch 35/50
----------
train loss: 0.0330 acc: 0.9894
val loss: 0.2182 acc: 0.9469

Epoch 36/50
----------
train loss: 0.0410 acc: 0.9873
val loss: 0.2592 acc: 0.9306

Epoch 37/50
----------
train loss: 0.0472 acc: 0.9869
val loss: 1.0702 acc: 0.7286

Epoch 38/50
----------
train loss: 0.0663 acc: 0.9824
val loss: 0.3419 acc: 0.9082

Epoch 39/50
----------
train loss: 0.0505 acc: 0.9861
val loss: 0.2064 acc: 0.9469

Epoch 40/50
----------
train loss: 0.0397 acc: 0.9857
val loss: 2.1660 acc: 0.6918

Epoch 41/50
----------
train loss: 0.0454 acc: 0.9873
val loss: 0.5105 acc: 0.8673

Epoch 42/50
----------
train loss: 0.0362 acc: 0.9898
val loss: 0.2108 acc: 0.9408

Epoch 43/50
----------
train loss: 0.0365 acc: 0.9873
val loss: 0.2131 acc: 0.9531

Epoch 44/50
----------
train loss: 0.0441 acc: 0.9841
val loss: 0.2501 acc: 0.9327

Epoch 45/50
----------
train loss: 0.0215 acc: 0.9959
val loss: 0.1597 acc: 0.9571

Epoch 46/50
----------
train loss: 0.0268 acc: 0.9927
val loss: 0.2004 acc: 0.9469

Epoch 47/50
----------
train loss: 0.0341 acc: 0.9902
val loss: 0.5767 acc: 0.8469

Epoch 48/50
----------
train loss: 0.0502 acc: 0.9845
val loss: 0.2257 acc: 0.9510

Epoch 49/50
----------
train loss: 0.0420 acc: 0.9869
val loss: 0.2313 acc: 0.9367

Epoch 50/50
----------
train loss: 0.0350 acc: 0.9890
val loss: 0.3690 acc: 0.9061

Training complete in 29m 16s
Best val Acc: 0.957143

Test Metrics:
[[261   1   3   1   1   3  10]
 [  8 258   8   4   1   1   0]
 [  5   2 265   1   0   7   0]
 [  0   0   2 278   0   0   0]
 [  0   3   7   0 269   1   0]
 [ 16   6   7   0   2 249   0]
 [ 12   0   3   0   2   0 263]]
              precision    recall  f1-score   support

           0       0.86      0.93      0.90       280
           1       0.96      0.92      0.94       280
           2       0.90      0.95      0.92       280
           3       0.98      0.99      0.99       280
           4       0.98      0.96      0.97       280
           5       0.95      0.89      0.92       280
           6       0.96      0.94      0.95       280

    accuracy                           0.94      1960
   macro avg       0.94      0.94      0.94      1960
weighted avg       0.94      0.94      0.94      1960

